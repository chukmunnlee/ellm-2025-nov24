{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Question and Answers\n",
    "In this workshop, you will learning how to write prompts and feed them into LLMs. You\n",
    "will also be learning how to use different prompt techniques to improve the response\n",
    "from the LLM.\n",
    "\n",
    "## Loading and Explorng the Dataset\n",
    "The workshop will be using [`facebook/ExploreToM`](https://huggingface.co/datasets/facebook/ExploreToM) dataset from [HuggingFace](https://huggingface.co)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the following libraries: datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset_name = \"facebook/ExploreToM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load and explore the dataset\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (13309, 18)}\n",
      "dict_keys(['train'])\n",
      "{'story_structure': Value('string'), 'infilled_story': Value('string'), 'question': Value('string'), 'expected_answer': Value('string'), 'qprop=params': Value('string'), 'qprop=nth_order': Value('int64'), 'qprop=non_unique_mental_state': Value('bool'), 'sprop=is_false_belief_story_1st': Value('bool'), 'sprop=is_false_belief_story_1st_and_2nd': Value('bool'), 'sprop=story_accuracy_1st_raw': Value('float64'), 'sprop=story_accuracy_1st_infilled': Value('float64'), 'sprop=global_idx': Value('int64'), 'param=story_type': Value('string'), 'param=num_stories_total': Value('int64'), 'param=max_sentences': Value('int64'), 'param=num_people': Value('int64'), 'param=num_moves': Value('int64'), 'param=num_rooms': Value('int64')}\n",
      "k: story_structure\n",
      "v: Mia entered the hospital staff lounge. Amelia entered the hospital staff lounge. Mia told privately to Madison about the hospital budget cuts. Madison entered the hospital staff lounge. Madison told privately to Mia about the accreditation requirements. Jasmine entered the hospital staff lounge. Jasmine told privately to Amelia about the accreditation requirements.\n",
      "k: infilled_story\n",
      "v: The hospital staff lounge, a small oasis amidst the bustling hospital corridors, was filled with the aroma of stale coffee and the soft hum of the refrigerator. The lounge's worn furniture and faded walls seemed to bear witness to countless conversations and late-night shifts, a backdrop for moments of rest and refuge. Mia slipped into the hospital staff lounge, her eyes scanning the room for a glimpse of the latest hospital bulletin, and Amelia followed closely, her worn shoes quiet on the scuffed linoleum floor. Mia discreetly tugged Madison into a private conversation, the topic of their hushed discussion immediately evident in the looks exchanged between them - concerned expressions that contrasted with the humdrum atmosphere of the staff lounge. The hospital staff lounge's scuffed linoleum floor creaked softly as Madison entered, her gaze drifting towards the usual gathering spots where colleagues shared stories and advice. A flutter of concern danced across Mia's face as Madison confided the details of the looming accreditation requirements, each phrase spilling into the next in a low, urgent cadence. The worn walls of the hospital staff lounge seemed to fade, leaving only the space between them, charged with an unspoken sense of uncertainty. Jasmine's confident stride echoed through the staff lounge as she made her entrance, her eyes swiftly scanning the room. A silent understanding flashed between Jasmine and Amelia as they moved into a corner of the lounge, far enough away from the other hospital staff to converse discreetly; it was here that Jasmine chose to brief Amelia on the nitty-gritty of the accreditation process, offering reassurances as much as concrete facts.\n",
      "k: question\n",
      "v: What does Madison think about Amelia's belief on accreditation requirements? (knows about it / does not know about it)\n",
      "k: expected_answer\n",
      "v: does not know about it\n",
      "k: qprop=params\n",
      "v: (['Madison', 'Amelia'], 'accreditation requirements', '<knowledge>-False')\n",
      "k: qprop=nth_order\n",
      "v: 2\n",
      "k: qprop=non_unique_mental_state\n",
      "v: True\n",
      "k: sprop=is_false_belief_story_1st\n",
      "v: True\n",
      "k: sprop=is_false_belief_story_1st_and_2nd\n",
      "v: True\n",
      "k: sprop=story_accuracy_1st_raw\n",
      "v: 0.875\n",
      "k: sprop=story_accuracy_1st_infilled\n",
      "v: 0.5\n",
      "k: sprop=global_idx\n",
      "v: 274\n",
      "k: param=story_type\n",
      "v: fantom-private\n",
      "k: param=num_stories_total\n",
      "v: 10\n",
      "k: param=max_sentences\n",
      "v: 15\n",
      "k: param=num_people\n",
      "v: 4\n",
      "k: param=num_moves\n",
      "v: 3\n",
      "k: param=num_rooms\n",
      "v: 1\n"
     ]
    }
   ],
   "source": [
    "# TODO: number of rows in the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# TODO: Keys in the dataset\n",
    "print(dataset.keys())\n",
    "\n",
    "# TODO: Feature names\n",
    "print(dataset['train'].features)\n",
    "\n",
    "# TODO: Display a single row\n",
    "idx = 5000\n",
    "for k, v in dataset['train'][idx].items():\n",
    "   print(f'k: {k}')\n",
    "   print(f'v: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import pipeline\n",
    "from transformers import pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pipeline`\n",
    "[`pipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines) is an easy to use API to perform inferencing. It provides a wrapper for task-specific pipelines and abstracts most of the complexity by allowing you to focus on the model and the task. \n",
    "\n",
    "You can use `pipeline` to perform summarisation, image classification, audio generation, etc. You can find an exhaustive list of `pipeline` task [here](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# TODO: Summarise the text with the pipeline's default model\n",
    "qna = pipeline('question-answering')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hospital staff lounge, a small oasis amidst the bustling hospital corridors, was filled with the aroma of stale coffee and the soft hum of the refrigerator. The lounge's worn furniture and faded walls seemed to bear witness to countless conversations and late-night shifts, a backdrop for moments of rest and refuge. Mia slipped into the hospital staff lounge, her eyes scanning the room for a glimpse of the latest hospital bulletin, and Amelia followed closely, her worn shoes quiet on the scuffed linoleum floor. Mia discreetly tugged Madison into a private conversation, the topic of their hushed discussion immediately evident in the looks exchanged between them - concerned expressions that contrasted with the humdrum atmosphere of the staff lounge. The hospital staff lounge's scuffed linoleum floor creaked softly as Madison entered, her gaze drifting towards the usual gathering spots where colleagues shared stories and advice. A flutter of concern danced across Mia's face as Madison confided the details of the looming accreditation requirements, each phrase spilling into the next in a low, urgent cadence. The worn walls of the hospital staff lounge seemed to fade, leaving only the space between them, charged with an unspoken sense of uncertainty. Jasmine's confident stride echoed through the staff lounge as she made her entrance, her eyes swiftly scanning the room. A silent understanding flashed between Jasmine and Amelia as they moved into a corner of the lounge, far enough away from the other hospital staff to converse discreetly; it was here that Jasmine chose to brief Amelia on the nitty-gritty of the accreditation process, offering reassurances as much as concrete facts.\n",
      "What does Madison think about Amelia's belief on accreditation requirements? (knows about it / does not know about it)\n",
      "--------------------------\n",
      "{'score': 0.11891406029462814, 'start': 1658, 'end': 1705, 'answer': 'offering reassurances as much as concrete facts'}\n",
      "does not know about it\n"
     ]
    }
   ],
   "source": [
    "idx = 5000\n",
    "story = dataset['train'][idx]['story_structure']\n",
    "story = dataset['train'][idx]['infilled_story']\n",
    "question = dataset['train'][idx]['question']\n",
    "answer = dataset['train'][idx]['expected_answer']\n",
    "\n",
    "predicted_answer = qna(question=question, context=story)\n",
    "print(story)\n",
    "print(question)\n",
    "print('--------------------------')\n",
    "print(predicted_answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inference - Question and Answer\n",
    "In this section, we will look at what `pipeline` does under the hood to perform its inference. This will give us a better understanding of the major steps involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT base cased distilled SQuAD\n",
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. More details [here](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   157, 14640,   188, 24537,  1136,  1294,   170,  3395,  1107,\n",
      "          1103,  1176,  1757,  1104,  1103,  1769,  1713,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encode text\n",
    "message = \"Big black bug bleeds black blood\"\n",
    "message = \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light\"\n",
    "message = \"Thou shalt not make a machine in the likeness of the human mind.\"\n",
    "\n",
    "# pt - PyTorch Tensors\n",
    "encoded_message = tokenizer(message, return_tensors='pt')\n",
    "\n",
    "print(encoded_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok = 101, dec = [CLS]\n",
      "tok = 157, dec = T\n",
      "tok = 14640, dec = ##hou\n",
      "tok = 188, dec = s\n",
      "tok = 24537, dec = ##halt\n",
      "tok = 1136, dec = not\n",
      "tok = 1294, dec = make\n",
      "tok = 170, dec = a\n",
      "tok = 3395, dec = machine\n",
      "tok = 1107, dec = in\n",
      "tok = 1103, dec = the\n",
      "tok = 1176, dec = like\n",
      "tok = 1757, dec = ##ness\n",
      "tok = 1104, dec = of\n",
      "tok = 1103, dec = the\n",
      "tok = 1769, dec = human\n",
      "tok = 1713, dec = mind\n",
      "tok = 119, dec = .\n",
      "tok = 102, dec = [SEP]\n"
     ]
    }
   ],
   "source": [
    "for tok in encoded_message.input_ids[0]:\n",
    "   dec = tokenizer.decode(tok)\n",
    "   print(f'tok = {tok}, dec = {dec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou shalt not make a machine in the likeness of the human mind.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(encoded_message.input_ids[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2562,  1602, 15430, 24752,  1116,  1602,  1892,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   157, 14640,   188, 24537,  1136,  1294,   170,  3395,  1107,\n",
      "          1103,  1176,  1757,  1104,  1103,  1769,  1713,   119,   102],\n",
      "        [  101, 19082,   117,  1362,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encoding multiple texts\n",
    "messages = [\n",
    "   \"Big black bug bleeds black blood\",\n",
    "   \"Thou shalt not make a machine in the likeness of the human mind.\",\n",
    "   \"hello, world\"\n",
    "]\n",
    "\n",
    "encoded_messages = tokenizer(messages, return_tensors='pt', padding=True)\n",
    "print(encoded_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Decode text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "Create and instance of the Large Language Model (LLM). We will then create a simple\n",
    "prompt, tokenize the prompt and feed the tokenized prompt to the LLM. The response\n",
    "from the LLM will be decoded to human friendly text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load libraries\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load question answer model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode context and question\n",
    "idx = 10\n",
    "story = dataset['train'][idx]['story_structure']\n",
    "story = dataset['train'][idx]['infilled_story']\n",
    "question = dataset['train'][idx]['question']\n",
    "answer = dataset['train'][idx]['expected_answer']\n",
    "\n",
    "enc_question = tokenizer(question, return_tensors='pt')\n",
    "enc_story = tokenizer(story, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] In which container was the pocket watch at the beginning? [SEP] The bustling theater was a hive of activity on this chilly autumn evening, its worn wooden floors and faded velvet curtains a testament to years of countless performances. The dimly lit green room, tucked away backstage, was a cozy refuge from the chaos, its plush armchairs and ornate wooden chest offering a warm respite for those who needed it. Dylan slipped away from the crowd, disappearing behind a tattered curtain as he made his way to a more secluded space. The soft glow of a floor lamp in the green room enveloped him, providing a sense of tranquility amidst the pre - show chaos. Dylan carefully placed the pocket watch in the ornate wooden chest, hidden from view, and Clayton caught a glimpse of this sneaky maneuver from his secret vantage point. Clayton ' s eyes narrowed as he pushed open the creaky door and stepped into the green room, the sudden movement making the softly lit space seem almost anticipatory. In a swift motion, Dylan delved into the leather satchel, repositioning its contents to accommodate the pocket watch, which now nestled among its weathered confines in the green room. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Tokenize the inputs\n",
    "enc_input = tokenizer(question, story, return_tensors='pt', padding=True)\n",
    "#print(enc_input)\n",
    "print(tokenizer.decode(enc_input.input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -3.0070,  -7.8348,  -7.9082,  -7.7894,  -9.4917,  -9.4841,  -8.6417,\n",
      "          -9.5561,  -9.0523,  -8.8516,  -8.2852,  -6.7598,  -8.1208,  -6.9686,\n",
      "          -7.6955, -10.2469,  -8.1907, -11.4937, -10.5607,  -9.9994, -11.1921,\n",
      "         -11.5424,  -9.2086, -10.3368,  -8.9562,  -7.6995, -10.7880,  -7.8180,\n",
      "          -9.4568, -10.5448,  -8.5158,  -7.3693,  -6.9435,  -8.5076, -10.6069,\n",
      "          -7.0253,  -4.7177,  -7.5203, -10.9336, -10.1342, -10.9254, -11.5466,\n",
      "          -9.6855, -11.6445,  -9.5176,  -9.1132,  -9.6704,  -4.7700,  -5.2772,\n",
      "          -9.2881,  -6.9881,  -4.4925,  -7.0064,  -9.5313,  -7.8890,  -9.7348,\n",
      "          -7.4000,  -9.4579,  -9.3632,  -8.9741,  -8.1093,  -9.5967,  -8.1028,\n",
      "         -11.1570, -10.3856,  -8.6170, -10.5958,  -7.4434,  -7.4832,  -9.7634,\n",
      "          -7.7349,  -9.4229,  -9.5982,  -9.6697,  -5.1695,  -4.7794,  -6.2363,\n",
      "         -10.1923, -10.3071,  -8.8503, -10.3421, -11.0937, -10.9019, -11.7270,\n",
      "         -10.2299, -11.3276, -10.1493, -10.3330,  -9.9043,  -6.2169,  -9.5730,\n",
      "         -10.7695, -11.1483, -10.3524,  -9.0404, -10.9747,  -8.2484,  -9.0920,\n",
      "          -7.6247,  -6.7569,  -8.9341,  -7.9577, -10.8313,  -9.4735,  -9.7055,\n",
      "          -9.8479, -10.5536,  -9.8362,  -8.0279,  -8.4763,  -7.2532,  -9.3134,\n",
      "          -8.2671,  -8.9522,  -4.4602,  -4.7406,  -6.4781,  -7.9754,  -1.3953,\n",
      "          -1.7290,  -4.6164,  -5.9069,  -5.0659,  -4.0263,  -6.6114,  -8.4426,\n",
      "         -10.1441,  -9.0132,  -9.2415,  -8.5927,  -9.4802,  -9.2641, -10.6981,\n",
      "          -8.0869, -10.4824, -10.9781, -10.0343,  -9.4234,  -9.1582,  -7.1678,\n",
      "         -11.1170,  -9.3100,  -7.8504,  -7.3324,  -3.1153,  -5.7555,  -4.6733,\n",
      "          -3.9982,  -3.1213,  -5.9126,   1.5243,   4.4349,   4.3147,   4.3622,\n",
      "           1.4555,  -5.6447,  -3.8606,  -9.0615,  -6.8553,  -9.0998,  -9.5542,\n",
      "          -5.3437,  -8.8273,  -9.2488,  -8.8069, -10.0826,  -7.7339,  -7.3587,\n",
      "         -10.2459,  -8.6802,  -9.1592,  -7.4794,  -5.5425,  -6.3721,  -8.3780,\n",
      "          -8.7551,  -8.9597,  -5.7138, -10.2255,  -9.9905,  -9.2310,  -8.9187,\n",
      "          -9.7887,  -9.0202,  -8.2937,  -8.2492,  -5.5902,  -5.2377,  -8.2566,\n",
      "          -8.5109,  -6.6071, -10.8988,  -8.6773,  -8.6527,  -6.6996,  -5.7058,\n",
      "          -8.1812, -10.0957,  -8.5827,  -9.0099,  -9.7259, -10.1724,  -8.6828,\n",
      "          -7.7856,  -9.2071,  -8.8424, -11.3361, -10.4989,  -9.1040, -10.8123,\n",
      "         -10.9211, -10.2907,  -9.9376,  -4.9786,  -8.0404,  -8.0107,  -9.7252,\n",
      "          -8.9776,  -4.4378,  -6.5163,  -9.5295,  -5.6289,   1.9134,   2.3534,\n",
      "          -1.0634,  -3.8364,  -8.6888,  -6.0147,  -9.0836,  -9.4423,  -6.2017,\n",
      "          -6.6993,  -8.1646,  -7.4513,  -5.7745,  -4.8744,  -6.9714,  -9.5278,\n",
      "          -9.1271,  -8.8106,  -8.8684,  -8.6568,  -8.0207,  -6.7044,  -9.7663,\n",
      "          -7.4857,  -9.0250,  -9.4071,  -8.1283,  -6.0102,  -8.4182,  -9.0684,\n",
      "          -8.1208]], grad_fn=<CloneBackward0>), end_logits=tensor([[ -2.3431,  -8.7239,  -8.0643,  -7.0043,  -9.4790, -11.0195, -10.1355,\n",
      "          -8.2119,  -9.9851,  -9.5490,  -7.3325,  -7.4391,  -8.6069,  -9.3780,\n",
      "          -9.5883,  -8.9872,  -7.0308, -10.9585, -11.2629, -11.3470,  -8.7087,\n",
      "         -11.2829,  -8.3137, -10.5102,  -9.7360,  -8.9627,  -9.0720,  -7.7570,\n",
      "          -7.2559,  -8.2706, -10.3594,  -8.6144,  -7.6075,  -6.4550, -10.4445,\n",
      "          -8.8659,  -5.6221,  -5.5468, -11.3758, -10.6944,  -9.5634, -10.9810,\n",
      "          -9.5667, -11.1741,  -9.5016,  -7.6761,  -8.3523,  -8.5216,  -8.6181,\n",
      "          -8.7760,  -6.7137,  -6.1247,  -3.9986,  -6.7389,  -8.7570,  -8.4553,\n",
      "          -5.2412,  -6.7751, -10.2912, -10.3497, -10.1831,  -7.5043,  -7.4955,\n",
      "         -10.6863, -10.6563,  -6.4943,  -7.6017, -10.0900,  -9.6493,  -8.3018,\n",
      "          -9.2312,  -8.5721,  -6.0584,  -9.5467,  -7.8366,  -6.2366,  -3.9329,\n",
      "         -10.8187, -11.0523,  -9.0856, -10.1763,  -9.6577,  -7.7457, -10.6537,\n",
      "         -10.2625, -10.8825,  -9.4074,  -6.9578,  -7.5728,  -6.5878,  -9.7811,\n",
      "          -9.3807, -10.2362, -10.3500,  -6.4561,  -7.5106,  -8.9147,  -9.0307,\n",
      "          -8.7288,  -9.3050,  -6.1830,  -4.8880,  -9.3144,  -9.3482,  -9.8503,\n",
      "          -9.5577,  -9.0121, -10.2062,  -9.8734,  -9.6598,  -9.3371,  -6.7961,\n",
      "          -5.1708,  -5.2832,  -8.8941,  -8.0846,  -6.8854,  -9.2746,  -7.0215,\n",
      "          -5.7270,  -0.5755,  -8.1408,  -7.7123,  -5.2752,  -1.1932,  -8.5406,\n",
      "          -8.1707,  -5.4973,  -5.0538, -10.0750, -10.6045,  -9.5568, -10.6267,\n",
      "          -9.7891,  -9.6107,  -9.3998,  -6.4616,  -9.2224,  -9.7772,  -8.9582,\n",
      "          -9.6519,  -7.5505,  -4.9923,  -7.0091,  -4.4469,  -6.8401,  -6.2605,\n",
      "          -8.3923,  -6.5195,  -3.3267,  -6.2783,  -4.9890,  -2.7559,  -0.7408,\n",
      "           6.0687,   0.8937,  -5.4922,  -9.4126,  -0.5852,  -3.2738,  -7.2350,\n",
      "          -6.0868,  -9.2956,  -9.9142,  -7.4560,  -9.7390,  -8.6345,  -8.6576,\n",
      "          -7.3516,  -5.1082,  -9.1291,  -7.9015,  -6.3914,  -7.7552,  -5.2693,\n",
      "          -2.5741,  -4.6425,  -7.4056,  -9.3495,  -8.2062,  -7.9948,  -7.4984,\n",
      "          -8.9250,  -9.1876,  -8.8756,  -7.9115,  -8.2080,  -8.3371,  -7.2268,\n",
      "          -5.5958,  -3.0724,  -8.5618,  -7.9945,  -8.5085,  -8.6530,  -6.6568,\n",
      "          -3.0847,  -5.9018, -10.3518,  -9.5010,  -7.9498, -10.5662, -10.3897,\n",
      "          -9.4520,  -8.2558,  -6.9979, -10.2345, -10.6789, -10.5408,  -9.9882,\n",
      "          -9.4281,  -6.0890,  -6.8407,  -9.9390, -10.4310,  -9.3849,  -6.7786,\n",
      "          -7.2320,  -5.8901,  -8.8812,  -6.7263,  -7.8494,  -6.0320,  -2.0826,\n",
      "          -5.0928,   4.3525,  -4.2841,  -9.2008,  -8.1617,  -7.4930,  -8.9510,\n",
      "          -3.9668,  -9.2540,  -7.0568,  -8.9940,  -7.5180,  -3.4107,  -4.9721,\n",
      "          -9.2459,  -9.9223,  -8.6941,  -9.6691,  -9.3950,  -8.9237,  -7.3558,\n",
      "          -9.1798,  -4.6840, -10.1023,  -9.3585,  -6.7195,  -2.6121,  -3.3602,\n",
      "          -8.6068]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Pass the encoded question and story to the QnA model\n",
    "result = model(enc_input.input_ids, enc_input.attention_mask)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['start_logits', 'end_logits'])\n",
      "start = 151\n",
      "end = 155\n",
      "tensor([ 1103, 19870,  4122,  2229])\n",
      "the ornate wooden chest\n"
     ]
    }
   ],
   "source": [
    "print(result.keys())\n",
    "\n",
    "start_pos = torch.argmax(result.start_logits)\n",
    "end_pos = torch.argmax(result.end_logits) + 1\n",
    "enc_ans = enc_input.input_ids[0][start_pos: end_pos]\n",
    "\n",
    "print(f'start = {start_pos}')\n",
    "print(f'end = {end_pos}')\n",
    "print(enc_ans)\n",
    "\n",
    "predicted_answer = tokenizer.decode(enc_ans)\n",
    "print(predicted_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellm-2025-nov24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
